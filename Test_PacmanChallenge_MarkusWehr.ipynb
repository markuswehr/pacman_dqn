{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, InputLayer, BatchNormalization\n",
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import rl\n",
    "#from rl.layers import NoisyNetDense\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import GreedyQPolicy\n",
    "from keras import initializers\n",
    "import keras.engine\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import collections, itertools\n",
    "\n",
    "import gym\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDense(Dense):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.output_dim = units\n",
    "        super(NoisyDense, self).__init__(units, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        self.input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=None,\n",
    "                                      constraint=None)\n",
    "\n",
    "        # Zweiter Kernel (trainable weights) für Steuerung des Zufalls.\n",
    "        self.kernel_sigma = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                      initializer=initializers.Constant(0.017),\n",
    "                                      name='sigma_kernel',\n",
    "                                      regularizer=None,\n",
    "                                      constraint=None)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=None,\n",
    "                                        constraint=None)\n",
    "\n",
    "            # trainable, Steuerung des Zufalls des Bias.\n",
    "            self.bias_sigma = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=initializers.Constant(0.017),\n",
    "                                        name='bias_sigma',\n",
    "                                        regularizer=None,\n",
    "                                        constraint=None)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: self.input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Erzeugen der Matrix mit Zufallszahlen (bei jedem Aufruf neu erzeugt) - Vektor-Version\n",
    "        # (siehe Noisy Nets Paper) wäre effizienter.\n",
    "        self.kernel_epsilon = K.random_normal(shape=(self.input_dim, self.units))\n",
    "\n",
    "        w = self.kernel + K.tf.multiply(self.kernel_sigma, self.kernel_epsilon)\n",
    "        output = K.dot(inputs, w)\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Erzeugung Zufallsvektor für Bias-Zufall.\n",
    "            self.bias_epsilon = K.random_normal(shape=(self.units,))\n",
    "\n",
    "            b = self.bias + K.tf.multiply(self.bias_sigma, self.bias_epsilon)\n",
    "            output = output + b\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, dense_layers, dense_units):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    model.add(NoisyDense(units=dense_units, activation='relu'))\n",
    "    #model.add(NoisyDense(units=dense_units, activation='relu'))\n",
    "    for i in range(dense_layers):\n",
    "        #model.add(NoisyDense(units=dense_units, activation='relu'))\n",
    "        model.add(Dense(units=dense_units, activation='relu'))\n",
    "    model.add(NoisyDense(units=dense_units, activation='relu'))\n",
    "    for i in range(dense_layers):\n",
    "        #model.add(NoisyDense(units=dense_units, activation='relu'))\n",
    "        model.add(Dense(units=dense_units, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = obs.shape\n",
    "nb_actions = env.action_space.n  # 9\n",
    "dense_layers = 5\n",
    "dense_units = 256\n",
    "\n",
    "online_network = create_dqn_model(input_shape, nb_actions, dense_layers, dense_units)\n",
    "online_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = './Competition/MsPacman_DQN_9/weights'\n",
    "online_network.load_weights(os.path.join(weights_folder, 'weights_last.h5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngames = 100\n",
    "eps = 0.05\n",
    "render = False\n",
    "\n",
    "scores = test_dqn(ngames, online_network, eps=eps, render=render)\n",
    "\n",
    "print('\\nMean score: ', np.mean(scores))\n",
    "print('\\nMax score: ', np.max(scores))\n",
    "print('\\nFifth percentile: ',np.percentile(scores, 95))\n",
    "print('\\nPercentiles:')\n",
    "print([ np.percentile(scores, p) for p in [0, 25, 50, 75, 100] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "ngames = 5\n",
    "eps = 0.05\n",
    "render = True\n",
    "\n",
    "scores = test_dqn(ngames, online_network, eps=eps, render=render)\n",
    "\n",
    "print('\\nMean score: ', np.mean(scores))\n",
    "print('\\nMax score: ', np.max(scores))\n",
    "print('\\nPercentiles:')\n",
    "print([ np.percentile(scores, p) for p in [0, 25, 50, 75, 100] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
